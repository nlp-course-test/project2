{"cells": [{"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "view-in-github"}, "source": "<a href=\"https://colab.research.google.com/github/nlp-course/materials/blob/tmp_psets/distrib/project2/project2_sequence.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "zXQlsJiAboxU"}, "source": "# Project 2: Sequence labeling \u2013 The slot filling task\n\nThe second segment of the project involves a sequence labeling task, in which the goal is to label the tokens in a text. Many NLP tasks have this general form. Most famously is the task of _part-of-speech labeling_, where the tokens in a text are to be labeled with their part of speech (noun, verb, preposition, etc.).\n\nIn this segment, you'll implement a system for filling the slots in a template that is intended to describe the meaning of an ATIS query. For instance, the sentence \n\n    What's the earliest arriving flight between Boston and Washington DC?\n    \nmight be associated with the following slot-filled template: \n\n    flight_id\n        fromloc.cityname: boston\n        toloc.cityname: washington\n        toloc.state: dc\n        flight_mod: earliest arriving\n    \nYou may wonder how this task is a sequence labeling task. We label each word in the source sentence with a tag taken from a set of tags that correspond to the slot-labels. For each slot-label, say `flight_mod`, there are two tags: `B-flight_mod` and `I-flight_mod`. These are used to mark the beginning (B) or interior (I) of a phrase that fills the given slot. In addition, there is a tag for other (O) words that are not used to fill any slot. Thus the sample sentence would be labeled as follows:\n\n| Token   | Label    |\n| ------- | ------ | \n| BOS | O |\n| what's | O |\n| the | O |\n| earliest | B-flight_mod |\n| arriving | I-flight_mod |\n| flight | O |\n| between | O |\n| boston | B-fromloc.city_name |\n| and | O |\n| washington | B-toloc.city_name |\n| dc | B-toloc.state_code |\n| EOS | O |\n\n`BOS` and `EOS` are special tokens to indicate the beginning and end of the sentence. The template itself is associated with the question type for the sentence, perhaps as recovered from the sentence in the last project segment.\n\nIn this segment, you'll implement two methods for sequence labeling: a hidden Markov model (HMM) and a recurrent neural network (RNN). By the end of this homework, you should have grasped the pros and cons of both approaches.\n\n## Goals\n\n1. Implement an HMM-based approach to sequence labeling.\n2. Implement an RNN-based approach to sequence labeling.\n3. Implement an LSTM-based approach to sequence labeling.\n4. (Optional) Compare the performances of HMM and RNN/LSTM under different amount of training data. Discuss the pros and cons of the HMM approach and the neural approach."}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "L3MDY8fqdYQg"}, "source": "## Setup"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "mgumU2YoUtQn"}, "outputs": [], "source": "import copy\nimport math\nimport random\n\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torchtext as tt\n\nimport matplotlib.pyplot as plt\n\n# Set random seeds\nseed = 1234\nrandom.seed(seed)\ntorch.manual_seed(seed)\n\n# GPU check, sets runtime type to \"GPU\" where available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint (device)"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "RN3cNFRmvRgS"}, "source": "### Load Data\n\nFirst, we download the ATIS dataset."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "tr1l3mMyfZ4u"}, "outputs": [], "source": "!wget -nv -N -P data https://raw.githubusercontent.com/nlp-course/data/master/ATIS/atis.train.txt\n!wget -nv -N -P data https://raw.githubusercontent.com/nlp-course/data/master/ATIS/atis.dev.txt\n!wget -nv -N -P data https://raw.githubusercontent.com/nlp-course/data/master/ATIS/atis.test.txt"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "PvzFJtVlf7R-"}, "source": "### Data Preprocessing\n\nWe again use `torchtext` to load data and convert words to indices in the vocabulary. We use one field `TEXT` for processing the question, and another field `TAG` for processing the sequence labels.\n\nWe treat words occurring fewer than three times in the training data as _unknown words_. They'll be replaced by the unknown word token `<unk>`. "}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "RHSSr75vgFR4"}, "outputs": [], "source": "MIN_FREQ = 3\n\nTEXT = tt.data.Field(init_token=\"<bos>\", batch_first=True)\nTAG = tt.data.Field(init_token=\"<bos>\", batch_first=True)\nfields=(('text', TEXT), ('tag', TAG))\n\ntrain, val, test = tt.datasets.SequenceTaggingDataset.splits(fields=fields, \n            path='./data/', train='atis.train.txt', validation='atis.dev.txt',\n            test='atis.test.txt')\n\nTEXT.build_vocab(train.text, min_freq=MIN_FREQ)\nTAG.build_vocab(train.tag)\n\nprint (f\"Size of English vocabulary: {len(TEXT.vocab)}\")\nprint (f\"Most common English words: {TEXT.vocab.freqs.most_common(10)}\\n\")\n\nprint (f\"Number of tags: {len(TAG.vocab)}\")\nprint (f\"Most common tags: {TAG.vocab.freqs.most_common(10)}\")"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "Wb5BbrHf86SO"}, "source": "Note that we passed in `init_token=\"<bos>\"` for both fields, which essentially prepends the sequence of words and tags with `<bos>`. This relieves us from estimating the initial distribution of states in HMMs, since we always start from `<bos>`."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "HEuhnBXE8Xrf"}, "outputs": [], "source": "initial_state_str = TAG.init_token\ninitial_state_id = TAG.vocab.stoi[TAG.init_token]\nprint (f\"Initial state: {initial_state_str}\")\nprint (f\"Initial state id: {initial_state_id}\")"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "BbJ1HfJWgHJk"}, "source": "Now, we can iterate over the dataset using `torchtext`'s iterator. To simplify RNN implementation we use ***batch size 1*** throughout this project."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "MAIdJbAfgO4E"}, "outputs": [], "source": "BATCH_SIZE = 1 # for simplicity we use batch size 1\n\ntrain_iter, val_iter, test_iter = tt.data.BucketIterator.splits(\n    (train, val, test), batch_size=BATCH_SIZE, repeat=False, device=device)\n\nbatch = next(iter(train_iter))\n\nprint (f\"First batch of text: {batch.text[0]}\")\nprint (f\"Converted back to string: {[TEXT.vocab.itos[i] for i in batch.text[0]]}\")\n\nprint (f\"First batch of tags: {batch.tag}\")\nprint (f\"Converted back to string: {[TAG.vocab.itos[i] for i in batch.tag[0]]}\")"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "rrbmrJwiEszW"}, "source": "The goal of this project is to predict the sequence of tags `batch.tag` given a sequence of words `batch.text`."}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "OECrLzLtgf5o"}, "source": "## HMM for Sequence Labeling\n\n### Notation\n\nFirst, let's start with some notations. We use $Q = \\langle{Q_1, Q_2, \\ldots, Q_N} \\rangle$ to denote the possible tags, which is the state space of the HMM, and $\\mathcal{V} = \\langle v_1, v_2, \\ldots v_V \\rangle$ to denote the vocabulary of word types.  We use $q_{t}\\in Q$ to denote the state at time step $t$ (where $t$ varies from $1$ to $T$), and $o_t\\in \\mathcal{V}$ to denote the observation (word) at time step $t$."}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "XZL1KkJ2iO0p"}, "source": "### Training an HMM by counting\n\nRecall that an HMM is defined via a transition matrix $A$ which stores the probability of moving from one state $Q_i$ to another $Q_j$, that is, \n\n$$A_{ij}=P(q_{t+1}=Q_j  \\given  q_t=Q_i)$$\n\nand an emission matrix $B$ which stores the probability of generating word $\\mathcal{V}_j$ given state $Q_i$, that is, \n\n$$B_{ij}= P(o_t=\\mathcal{V}_j  \\given q_t= Q_i)$$\n\nIn our case, since the labels are observed in the training data, we can directly use counting to determine (maximum likelihood) estimates of $A$ and $B$."}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "FZfPy07kg8Bn"}, "source": "#### Goal 1 (a): Find the transition matrix\n\n$$\n   \\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n   \\newcommand{\\Prob}{{\\Pr}}\n   \\newcommand{\\given}{\\,|\\,}\n   \\newcommand{\\vect}[1]{\\mathbf{#1}}\n   \\newcommand{\\cnt}[1]{\\sharp(#1)}\n$$\nThe matrix $A$ contains the transition probabilities: $A_{ij}$ is the probability of moving from state $Q_i$ to state $Q_j$ in the training data, so that $\\sum^{N}_{j = 1 } A_{ij} = 1$ for all $i$. \n\nWe find these probabilities by counting the number of times state $Q_j$ appears right after state $Q_i$, as a proportion of all of the transitions from $Q_i$.\n\n$$\nA_{ij} = \\frac{\\cnt{Q_i, Q_j} + \\delta}{\\cnt{Q_i} + \\delta N}\n$$\n\n(In the above formula, we also used add-$\\delta$ smoothing.)\n\nUsing the above definition, implement the method `train_A` in the `HMM` class, which calculates and returns the $A$ matrix as a tensor of size $N \\times N$.\n\n> You'll want to go ahead and implement this part now, and test it below, before moving on to the next goal."}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "YKkywJsCY5PU"}, "source": "#### Goal 1(b): Find the emission matrix $B$\n\nSimilar to the transition matrix, the emission matrix contains the emission probabilities such that $B_{ij}$ is probability of word $o_t=\\mathcal{V}_j$ conditioned on state $q_t=Q_i$.\n\nWe can find this by counting as well.\n$$\nB_{ij} = \\frac{\\cnt{Q_i, \\mathcal{V}_j} + \\delta}{\\cnt{Q_i} + \\delta V}\n$$\n\nUsing the above definitions, implement the `train_B` method in the `HMM` class, which calculates and returns the $B$ matrix as a tensor of size $N \\times V$.\n\n> You'll want to go ahead and implement this part now, and test it below, before moving on to the next goal."}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "O9M5u0CpjkI0"}, "source": "### Sequence labeling with a trained HMM\n\nNow that we're able to train an HMM by estimating the transition matrix $A$ and the emission matrix $B$, we can apply it to the task of sequence labeling. Our goal is to find the most probable sequence of tags $\\hat q \\in Q^T$ given a sequence of words $o \\in \\mathcal{V}^T$.\n\n$$\n\\begin{align*}\n\\hat q &= \\operatorname*{argmax}\\limits_{q \\in Q^T}(P(q \\given o)) \\\\\n& = \\operatorname*{argmax}_{q \\in Q^T}(P(q,o)) \\\\\n& = \\operatorname*{argmax}_{q \\in Q^T}\\left(\\Pi^{T}_{t = 1} P(o_{t+1} \\given q_{t+1})P(q_{t+1} \\given q_t)\\right)\n\\end{align*}\n$$\nwhere $P(o_{t+1}=\\mathcal{V}_j \\given q_{t+1}=Q_i) = B_{ij}$, $P(q_{t+1}=Q_j \\given q_t=Q_{i})=A_{ij}$."}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "INnW0jImbK7N"}, "source": "#### Goal 1 (c): Viterbi algorithm\n\nImplement the `predict` method, which should use the Viterbi algorithm to find the most likely sequence of tags for a sequence of `words`.\n\n> You'll want to go ahead and implement this part now, and test it below, before moving on to the next goal.\n\n> Warning: It may take up to 30 minutes to tag the entire test set depending on your implementation. We highly recommend that you begin by experimenting with your code using a _very small subset_ of the dataset, say two or three sentences, ramping up from there.\n\n> Hint: Consider how to use vectorized computations where possible for speed."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "7xwV922gkvYu"}, "outputs": [], "source": "#TODO\nclass HMMTagger():\n  def __init__ (self, text, tag):\n    self.text = text\n    self.tag = tag\n    self.V = len(text.vocab.itos) # vocabulary size\n    self.N = len(tag.vocab.itos) # state space size\n    self.initial_state = tag.vocab.stoi[tag.init_token]\n  \n  def train_A(self, iterator, delta):\n    \"\"\"Stores A for training dataset `iterator ` and add-`delta` smoothing.\"\"\"\n    #TODO: Add your solution from Goal 1 (a) here.\n    #      The returned value should be a tensor for the $A$ matrix\n    #      of size N x N.\n    \"your code here\"\n    A = torch.zeros(self.N, self.N, device=device)\n    return A\n\n  def train_B(self, iterator, delta):\n    \"\"\"Stores B for training dataset `iterator ` and add-`delta` smoothing.\"\"\"\n    #TODO: Add your solution from Goal 1 (b) here.\n    #      The returned value should be a tensor for the $B$ matrix\n    #      of size N x V.\n    \"your code here\"\n    B = torch.zeros(self.N, self.V, device=device)\n    return B\n\n  def train_all(self, iterator, delta=0.01):\n    \"\"\"Stores A and B for training dataset `iterator`.\"\"\"\n    self.log_A = self.train_A(iterator, delta).log()\n    self.log_B = self.train_B(iterator, delta).log()\n    \n  def predict(self, words):\n    \"\"\"Returns the most likely sequence of tags for a sequence of `words`.\"\"\"\n    #TODO: Add your solution from Goal 1 (b) here.\n    #      The returned value should be a list of tag ids.\n    \"your code here\"\n    bestpath = []\n    return bestpath\n    \n  def evaluate(self, iterator):\n    \"\"\"Returns the model's performance on a given dataset `iterator`.\"\"\"\n    correct = 0\n    total = 0\n    for batch in tqdm(iterator):\n      words = batch.text[0]\n      tags = batch.tag[0]\n      tags_pred = self.predict(words)\n      for tag_gold, tag_pred in zip(tags, tags_pred):\n        total += 1\n        if tag_pred == tag_gold:\n          correct += 1\n    return correct/total"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "O19h1vI0nTyF"}, "source": "Putting everything together, you should expect a correct implementation to reach **90% accuracy** after running the following cell."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "Tq4SqhvN8m1r"}, "outputs": [], "source": "# Instantiate and train classifier\nhmm_tagger = HMMTagger(TEXT, TAG)\nhmm_tagger.train_all(train_iter)\n\n# Evaluate model performance\nprint(f'Training accuracy: {hmm_tagger.evaluate(train_iter):.3f}\\n'\n      f'Test accuracy:     {hmm_tagger.evaluate(test_iter):.3f}')"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "Qmi4Kl-woT9x"}, "source": "## RNN for Sequence Labeling\n\nHMMs work quite well for this sequence labeling task. Now let's take an alternative (and more trendy) approach: RNN/LSTM-based sequence labeling. Similar to the HMM part of this project, you will also need to train a model on the training data, and then use the trained model to decode and evaluate some testing data.\n\n\n\n![RNN Visualization](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Recurrent_neural_network_unfold.svg/2880px-Recurrent_neural_network_unfold.svg.png)\n\nAfter unfolding an RNN, the cell at time $t$ generates the observed output $o_t$ based on the input $x_t$ and the hidden state of the previous cell $h_{t-1}$, according to the following equations. (Here, the $o_t$ are the pre-softmax output logits.)\n\n$$\n\\begin{align*}\nh_t &=  U x_t + V h_{t-1} + b_h \\\\\no_t &= W h_t + b_y\n\\end{align*}\n$$\n\nThe parameters here are the elements of the matrices $U$, $V$, and $W$, and the bias terms $b_h$ and $b_y$. Similar to the last project segment, we will perform the forward computation, calculate the loss, and then perform the backward computation to compute the gradients with respect to these model parameters. Finally, we will adjust the parameters opposite the direction of the gradients to minimize the loss, repeating until convergence."}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "GcbFYlvWoaR_"}, "source": "#### Goal 2 (a): RNN training\n\nImplement the forward pass of the RNN tagger and the loss function using the  starter code below. The training and optimization code is already provided. You will be adding the below three methods:\n\n1. `__init__`: an initializer that takes two `torchtext` fields providing descriptions of the text and tag aspects of examples, as well as an `embedding_size` specifying the size of word embeddings and a `hidden_size` specifying the size of hidden states.\n\n2. `forward`: performs one step of RNN forward computation with current word `word` and previous hidden state `hidden`. This function is expected to return `output` which stores logits, and the current hidden state `hidden`. Since we only consider batch size 1, `word` is a tensor with 1 element, and `hidden` is a vector of size `hidden_size`. \n\n3. `compute_loss`: computes loss by comparing `output` returned by `forward` to `ground_truth` which stores the true tag id. `output` is a vector of size $N$ (recall that $N=|Q|$), and `ground_truth` is a tensor with a single element. Note that the criterion functions in `torch` expect batched outputs, so you might need to use `output.unsqueeze(0)` to convert it from a 1-D vector to a 2-D matrix (with a single row).\n\n> Hint: You might find [`nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html), [`nn.CrossEntropyLoss`](https://pytorch.org/docs/master/generated/torch.nn.CrossEntropyLoss.html) from the last project segment useful. Note that if you use `nn.CrossEntropyLoss` then you should not use a softmax layer at the end since that's already absorbed into the loss function. Alternatively, you can use [`nn.LogSoftmax`](https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html) as the final sublayer in the forward pass, but then you need to use [`nn.NLLLoss`](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html), which does not contain its own softmax. We recommend the former, since working in log space is usually more numerically stable."}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "y7SHhMr5pi_l"}, "source": "#### Goal 2 (b) RNN decoding\n\nImplement the method `predict` to tag a sequence of `words`. "}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "LnGserXgKSuU"}, "outputs": [], "source": "#TODO\nclass RNNTagger(nn.Module):\n  def __init__(self, text, tag, embedding_size, hidden_size):\n    super().__init__()\n    self.text = text\n    self.tag = tag\n    # Keep the vocabulary sizes available\n    self.N = len(tag.vocab.itos) # |Q|\n    self.V = len(text.vocab.itos)  # vocab_size\n    self.embedding_size = embedding_size\n    self.hidden_size = hidden_size\n    #TODO: implement below, create essential modules and loss function\n    raise NotImplementedError\n\n\n  def forward(self, word, hidden):\n    \"\"\"Performs one step of RNN forward, returns current output and hidden state.\n    \n    Arguments: \n      hidden the hidden state from the previous cell, \n      word   the current input word id\n    Returns:\n      a pair of the current output and the current hidden state\n    \"\"\"\n    # TODO: implement below\n    return output, hidden\n\n  def compute_loss(self, output, ground_truth):\n    \"\"\"Computes loss.\"\"\"\n    # TODO: implement below\n    return loss\n\n  def train_all(self, train_iter, val_iter, epochs=3, learning_rate=0.001):\n    # Switch the module to training mode\n    self.train()\n    # Use Adam to optimize the parameters\n    optim = torch.optim.Adam(self.parameters(), lr=learning_rate)\n    best_validation_accuracy = -float('inf')\n    best_model = None\n    # Run the optimization for multiple epochs\n    for epoch in range(epochs): \n      total = 0\n      running_loss = 0.0\n      for batch in tqdm(train_iter):\n        # Zero the parameter gradients\n        self.zero_grad()\n\n        # Input and target\n        words = batch.text[0] # vector with T word ids\n        tags = batch.tag[0]   # vector with T tags\n        \n        # Run forward pass and compute loss along the way.\n        hidden = torch.zeros(self.hidden_size, device=device)\n        loss = 0\n        for word, tag in zip(words, tags):\n          output, hidden = self(word, hidden)\n          loss = loss + self.compute_loss(output, tag)\n\n        # Perform backpropagation\n        loss.backward()\n        optim.step()\n\n        # Training stats\n        total += 1\n        running_loss += loss.item()\n        \n      # Evaluate and track improvements on the validation dataset\n      validation_accuracy = self.evaluate(val_iter)\n      if validation_accuracy > best_validation_accuracy:\n        best_validation_accuracy = validation_accuracy\n        self.best_model = copy.deepcopy(self.state_dict())\n      epoch_loss = running_loss / total\n      print (f'Epoch: {epoch} Loss: {epoch_loss:.4f} '\n             f'Validation accuracy: {validation_accuracy:.4f}')\n\n  def predict(self, words):\n    \"\"\"Returns the most likely sequence of tags for a sequence of `words`.\"\"\"\n    #TODO: implement below\n    tags = []\n    return tags\n\n  def evaluate(self, iterator):\n    \"\"\"Returns the model's performance on a given dataset `iterator`.\"\"\"\n    correct = 0\n    total = 0\n    for batch in tqdm(iterator):\n      words = batch.text[0]\n      tags = batch.tag[0]\n      tags_pred = self.predict(words)\n      for tag_gold, tag_pred in zip(tags, tags_pred):\n        total += 1\n        if tag_pred == tag_gold:\n          correct += 1\n    return correct/total"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "Q3ubVqd8EdoY"}, "source": "Run the below cell to train an RNN, and evaluate it. A proper implementation should reach **95%+ accuracy**."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "uIhOSOtU8m2d"}, "outputs": [], "source": "# Instantiate and train classifier\nrnn_tagger = RNNTagger(TEXT, TAG, embedding_size=36, hidden_size=36).to(device)\nrnn_tagger.train_all(train_iter, val_iter)\nrnn_tagger.load_state_dict(rnn_tagger.best_model)\n\n# Evaluate model performance\nprint(f'Training accuracy: {rnn_tagger.evaluate(train_iter):.3f}\\n'\n      f'Test accuracy:     {rnn_tagger.evaluate(test_iter):.3f}')"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "YCpYjn9W4yNK"}, "source": "## LSTM for Slot Filling\n\nDid your RNN perform better than HMM? How much better was it? Was that expected? RNNs tend to exhibit the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem). To remedy this, the Long-Short Term Memory (LSTM) model was introduced. In PyTorch, we can simply use [`nn.LSTM`](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html). "}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "WUMOAGzf3kTK"}, "source": "#### Goal 3 (a) Use LSTM instead of RNN\n\nUse LSTM instead of RNN, implement the `LSTMTagger` class with the below starter code. What are the expected input arguments of `nn.LSTM`?"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "1tdjAF0nq3hQ"}, "source": "#### Goal 3 (b) Use LSTM for decoding\n\nImplement the method `predict` to tag a sequence of `words`."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "llx63LPWKpgE"}, "outputs": [], "source": "#TODO\nclass LSTMTagger(nn.Module):\n  def __init__(self, text, tag, embedding_size, hidden_size):\n    super().__init__()\n    self.text = text\n    self.tag = tag\n    # Keep the vocabulary sizes available\n    self.N = len(tag.vocab.itos) # |Q|\n    self.V = len(text.vocab.itos)  # vocab_size\n    self.embedding_size = embedding_size\n    self.hidden_size = hidden_size\n\n    #TODO: implement below, create essential modules and loss function\n    raise NotImplementedError\n\n  def forward(self, word, hidden):\n    \"\"\"Performs one step of RNN forward, returns current output and hidden.\"\"\"\n    # TODO: implement below\n    return output, hidden\n\n  def compute_loss(self, output, ground_truth):\n    # TODO: implement below\n    return loss\n\n  def train_all(self, train_iter, val_iter, epochs=3, learning_rate=0.001):\n    # Switch the module to training mode\n    self.train()\n    # Use Adam to optimize the parameters\n    optim = torch.optim.Adam(self.parameters(), lr=learning_rate)\n    best_validation_accuracy = -float('inf')\n    best_model = None\n    # Run the optimization for multiple epochs\n    for epoch in range(epochs): \n      total = 0\n      running_loss = 0.0\n      for batch in tqdm(train_iter):\n        # Zero the parameter gradients\n        self.zero_grad()\n\n        # Input and target\n        words = batch.text[0] # vector with T word ids\n        tags = batch.tag[0]   # vector with T tags\n        \n        # Run forward pass and compute loss along the way.\n        hidden = torch.zeros(1, 1, self.hidden_size, device=device)\n        hidden = (hidden, hidden) # LSTM hidden is a tuple of two tensors\n        loss = 0\n        for word, tag in zip(words, tags):\n          output, hidden = self(word, hidden)\n          loss = loss + self.compute_loss(output, tag)\n\n        # Perform backpropagation\n        loss.backward()\n        optim.step()\n\n        # Training stats\n        total += 1\n        running_loss += loss.item()\n        \n      # Evaluate and track improvements on the validation dataset\n      validation_accuracy = self.evaluate(val_iter)\n      if validation_accuracy > best_validation_accuracy:\n        best_validation_accuracy = validation_accuracy\n        self.best_model = copy.deepcopy(self.state_dict())\n      epoch_loss = running_loss / total\n      print (f'Epoch: {epoch} Loss: {epoch_loss:.4f} '\n             f'Validation accuracy: {validation_accuracy:.4f}')\n\n  def predict(self, words):\n    \"\"\"Returns the most likely sequence of tags for a sequence of `words`.\"\"\"\n    #TODO: implement below\n    tags = []\n    return tags\n\n  def evaluate(self, iterator):\n    \"\"\"Returns the model's performance on a given dataset `iterator`.\"\"\"\n    correct = 0\n    total = 0\n    for batch in tqdm(iterator):\n      words = batch.text[0]\n      tags = batch.tag[0]\n      tags_pred = self.predict(words)\n      for tag_gold, tag_pred in zip(tags, tags_pred):\n        total += 1\n        if tag_pred == tag_gold:\n          correct += 1\n    return correct/total"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "b8c69PaVHymd"}, "source": "Run the below cell to train an LSTM, and evaluate it. A proper implementation should reach **95%+ accuracy**."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "Rg49kkS5G9GR"}, "outputs": [], "source": "# Instantiate and train classifier\nlstm_tagger = LSTMTagger(TEXT, TAG, embedding_size=36, hidden_size=36).to(device)\nlstm_tagger.train_all(train_iter, val_iter)\nlstm_tagger.load_state_dict(lstm_tagger.best_model)\n\n# Evaluate model performance\nprint(f'Training accuracy: {lstm_tagger.evaluate(train_iter):.3f}\\n'\n      f'Test accuracy:     {lstm_tagger.evaluate(test_iter):.3f}')"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "D43ge-OUrEw5"}, "source": "## (Optional) Goal 4: Compare HMM to RNN/LSTM with different amounts of training data\n\nVary the amount of training data and compare the performance of HMM to RNN or LSTM (Since RNN is similar to LSTM, picking one of them is enough.) Discuss the pros and cons of HMM and RNN/LSTM based on your experiments."}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "eCpu5MOorTD8"}, "source": "The below code shows how to subsample the training set with downsample ratio `ratio`. To speedup evaluation we only use 50 test samples."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "MHZq99iZlflx"}, "outputs": [], "source": "ratio = 0.1\ntest_size = 50\n\n# Set random seeds to make sure subsampling is the same for HMM and RNN\nrandom.seed(seed)\ntorch.manual_seed(seed)\n\ntrain, val, test = tt.datasets.SequenceTaggingDataset.splits(\n            fields=fields, \n            path='./data/', \n            train='atis.train.txt', \n            validation='atis.dev.txt',\n            test='atis.test.txt')\n\n# Subsample\nrandom.shuffle(train.examples)\ntrain.examples = train.examples[:int(math.floor(len(train.examples)*ratio))]\nrandom.shuffle(test.examples)\ntest.examples = test.examples[:test_size]\n\n# Rebuild vocabulary\nTEXT.build_vocab(train.text, min_freq=MIN_FREQ)\nTAG.build_vocab(train.tag)"}], "metadata": {"accelerator": "GPU", "colab": {"collapsed_sections": [], "include_colab_link": true, "name": "project2_sequence.ipynb", "provenance": []}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.3"}}, "nbformat": 4, "nbformat_minor": 4}
